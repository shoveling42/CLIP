{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LjxorbFbXXZ",
        "outputId": "4ee948b8-25de-4a81-c7a1-511af4cd0877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Indofashionclip'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 17 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (17/17), 6.30 KiB | 6.30 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shashnkvats/Indofashionclip.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAiBatjxbnI1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Indofashionclip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uwQ6QzUbwj3"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8R_QH_UeBZg",
        "outputId": "b5c015dd-6446-424b-b46f-666c27f89ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/mydrive; to attempt to forcibly remount, call drive.mount(\"/content/mydrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/mydrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCH8UbYTeHim"
      },
      "outputs": [],
      "source": [
        "# Dataset reference: https://www.kaggle.com/datasets/validmodel/indo-fashion-dataset\n",
        "!unzip -qq '/content/mydrive/MyDrive/Colab Notebooks/COSE474/archive.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5slQlEJ2i7PU"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gma1s8JOnqiJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import clip\n",
        "from transformers import CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNT2--UgnLzm"
      },
      "outputs": [],
      "source": [
        "# Choose computation device\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load pre-trained CLIP model\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86VZNwBjnOeb"
      },
      "outputs": [],
      "source": [
        "# Define a custom dataset\n",
        "class image_title_dataset():\n",
        "    def __init__(self, list_image_path,list_txt):\n",
        "        # Initialize image paths and corresponding texts\n",
        "        self.image_path = list_image_path\n",
        "        # Tokenize text using CLIP's tokenizer\n",
        "        self.title  = clip.tokenize(list_txt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Preprocess image using CLIP's preprocessing function\n",
        "        image = preprocess(Image.open(self.image_path[idx]))\n",
        "        title = self.title[idx]\n",
        "        return image, title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dItZEeS6nRGB"
      },
      "outputs": [],
      "source": [
        "# Create train dataloader\n",
        "json_path = '/content/Indofashionclip/train_data.json'\n",
        "image_path = '/content/Indofashionclip/images/train/'\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    input_data = []\n",
        "    for line in f:\n",
        "        obj = json.loads(line)\n",
        "        input_data.append(obj)\n",
        "\n",
        "list_image_path = []\n",
        "list_txt = []\n",
        "for item in input_data:\n",
        "  img_path = image_path + item['image_path'].split('/')[-1]\n",
        "  caption = item['class_label'][:40]\n",
        "  list_image_path.append(img_path)\n",
        "  list_txt.append(caption)\n",
        "\n",
        "dataset = image_title_dataset(list_image_path, list_txt)\n",
        "train_dataloader = DataLoader(dataset, batch_size=256, shuffle=True) # Define train dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create valid dataloader\n",
        "json_path = '/content/Indofashionclip/val_data.json'\n",
        "image_path = '/content/Indofashionclip/images/val/'\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    input_data = []\n",
        "    for line in f:\n",
        "        obj = json.loads(line)\n",
        "        input_data.append(obj)\n",
        "\n",
        "list_image_path = []\n",
        "list_txt = []\n",
        "for item in input_data:\n",
        "  img_path = image_path + item['image_path'].split('/')[-1]\n",
        "  caption = item['class_label'][:40]\n",
        "  list_image_path.append(img_path)\n",
        "  list_txt.append(caption)\n",
        "\n",
        "dataset = image_title_dataset(list_image_path, list_txt)\n",
        "val_dataloader = DataLoader(dataset, batch_size=256, shuffle=True) # Define valid dataloader"
      ],
      "metadata": {
        "id": "phqQJg-yku1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N77lwmCxnS1p"
      },
      "outputs": [],
      "source": [
        "# Function to convert model's parameters to FP32 format\n",
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()\n",
        "\n",
        "\n",
        "if device == \"cpu\":\n",
        "  model.float()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "6J4dPd-b8EC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"Set Your Project\", name=\"Set Your Log Name\")\n",
        "\n",
        "# Define variables to keep track of the best model and its corresponding loss\n",
        "best_loss = float('inf')\n",
        "best_model_path = 'best_model(30).pth'\n",
        "epoch_losses = []\n",
        "validation_losses = []  # List to store validation losses\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.2)\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "# Validation function with accuracy calculation\n",
        "def validate(model, dataloader, loss_img, loss_txt, device):\n",
        "    model.eval()\n",
        "    total_loss_img = 0.0\n",
        "    total_loss_txt = 0.0\n",
        "    correct_img = 0\n",
        "    correct_txt = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images, texts = batch\n",
        "            images = images.to(device)\n",
        "            texts = texts.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "            # Compute loss\n",
        "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "            loss_img_val = loss_img(logits_per_image, ground_truth)\n",
        "            loss_txt_val = loss_txt(logits_per_text, ground_truth)\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss_img += loss_img_val.item()\n",
        "            total_loss_txt += loss_txt_val.item()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            predicted_img = torch.argmax(logits_per_image, 1)\n",
        "            predicted_txt = torch.argmax(logits_per_text, 1)\n",
        "            correct_img += (predicted_img == ground_truth).sum().item()\n",
        "            correct_txt += (predicted_txt == ground_truth).sum().item()\n",
        "\n",
        "            total_samples += len(images)\n",
        "\n",
        "    avg_loss_img = total_loss_img / len(dataloader)\n",
        "    avg_loss_txt = total_loss_txt / len(dataloader)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy_img = correct_img / total_samples\n",
        "    accuracy_txt = correct_txt / total_samples\n",
        "\n",
        "    return (avg_loss_img + avg_loss_txt) / 2, accuracy_img, accuracy_txt\n",
        "\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 30\n",
        "for epoch in range(1, num_epochs):\n",
        "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch in pbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images, texts = batch\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "        # Compute loss\n",
        "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "        if device == \"cpu\":\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            convert_models_to_fp32(model)\n",
        "            optimizer.step()\n",
        "            clip.model.convert_weights(model)\n",
        "\n",
        "        epoch_loss += total_loss.item()\n",
        "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    epoch_losses.append(avg_epoch_loss)\n",
        "\n",
        "    # Validate the model on the validation set\n",
        "    val_loss, acc_img, acc_txt = validate(model, val_dataloader, loss_img, loss_txt, device)\n",
        "    validation_losses.append(val_loss)\n",
        "\n",
        "    # Save the best model\n",
        "    if avg_epoch_loss < best_loss:\n",
        "        best_loss = avg_epoch_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "    # Log metrics to WandB\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": avg_epoch_loss, \"validation_loss\": val_loss, \"img_acc\": acc_img, \"txt_acc\": acc_txt})\n",
        "\n",
        "# Print and save the best loss\n",
        "print(f\"Best Loss: {best_loss:.4f}\")\n",
        "with open('epoch_losses.txt', 'w') as f:\n",
        "    for epoch, loss in enumerate(epoch_losses):\n",
        "        f.write(f\"Epoch {epoch}/{num_epochs}, Loss: {loss:.4f}\\n\")\n",
        "\n",
        "# Save validation losses to a file\n",
        "with open('validation_losses.txt', 'w') as f:\n",
        "    for epoch, val_loss in enumerate(validation_losses):\n",
        "        f.write(f\"Epoch {epoch}/{num_epochs}, Validation Loss: {val_loss:.4f}\\n\")"
      ],
      "metadata": {
        "id": "cHfjRapOkk8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909,
          "referenced_widgets": [
            "6f38def8139846c3809fb86c0eda9aa1",
            "02c906e1302e44e88e2501e9968ffb5d",
            "9e5faefc33544acb931b773784d70804",
            "d68b01bfcfef4499bdb90b6e367d0b00",
            "a1cc08c1bc514f9085bb6a9eda3fcaac",
            "cbc3b0a01d494442bab92258091021ed",
            "16d3fbc3f73f494da9bedb184700387d",
            "b29859dd9e3f4ae4ad4b5b9bafe39aff"
          ]
        },
        "outputId": "1ac9ffa9-7239-4735-d423-28c6c81e3fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:wqqds89f) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f38def8139846c3809fb86c0eda9aa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>img_acc</td><td>█▁▁▁▁▁</td></tr><tr><td>loss</td><td>▁█████</td></tr><tr><td>txt_acc</td><td>█▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>img_acc</td><td>0.004</td></tr><tr><td>loss</td><td>5.54085</td></tr><tr><td>txt_acc</td><td>0.004</td></tr><tr><td>validation_loss</td><td>5.50638</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">epoch 30</strong> at: <a href='https://wandb.ai/behindstarter42/CLIP-2/runs/wqqds89f' target=\"_blank\">https://wandb.ai/behindstarter42/CLIP-2/runs/wqqds89f</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231202_132451-wqqds89f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:wqqds89f). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Indofashionclip/wandb/run-20231202_142209-82fmfxvm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/behindstarter42/CLIP-2/runs/82fmfxvm' target=\"_blank\">Init</a></strong> to <a href='https://wandb.ai/behindstarter42/CLIP-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/behindstarter42/CLIP-2' target=\"_blank\">https://wandb.ai/behindstarter42/CLIP-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/behindstarter42/CLIP-2/runs/82fmfxvm' target=\"_blank\">https://wandb.ai/behindstarter42/CLIP-2/runs/82fmfxvm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30, Loss: 1.9902: 100%|██████████| 357/357 [08:37<00:00,  1.45s/it]\n",
            "Epoch 2/30, Loss: 2.3770: 100%|██████████| 357/357 [08:39<00:00,  1.45s/it]\n",
            "Epoch 3/30, Loss: 2.0078: 100%|██████████| 357/357 [08:42<00:00,  1.46s/it]\n",
            "Epoch 4/30, Loss: 2.2480: 100%|██████████| 357/357 [08:38<00:00,  1.45s/it]\n",
            "Epoch 5/30, Loss: 2.2324: 100%|██████████| 357/357 [08:36<00:00,  1.45s/it]\n",
            "Epoch 6/30, Loss: 2.2500: 100%|██████████| 357/357 [08:42<00:00,  1.46s/it]\n",
            "Epoch 7/30, Loss: 2.2285: 100%|██████████| 357/357 [08:39<00:00,  1.46s/it]\n",
            "Epoch 8/30, Loss: 2.2148: 100%|██████████| 357/357 [08:31<00:00,  1.43s/it]\n",
            "Epoch 9/30, Loss: 2.0332: 100%|██████████| 357/357 [08:38<00:00,  1.45s/it]\n",
            "Epoch 10/30, Loss: 2.4023: 100%|██████████| 357/357 [09:04<00:00,  1.53s/it]\n",
            "Epoch 11/30, Loss: 2.3164: 100%|██████████| 357/357 [09:12<00:00,  1.55s/it]\n",
            "Epoch 12/30, Loss: 2.0898: 100%|██████████| 357/357 [09:11<00:00,  1.55s/it]\n",
            "Epoch 13/30, Loss: 2.2422: 100%|██████████| 357/357 [09:10<00:00,  1.54s/it]\n",
            "Epoch 14/30, Loss: 2.2578: 100%|██████████| 357/357 [09:08<00:00,  1.54s/it]\n",
            "Epoch 15/30, Loss: 2.1719: 100%|██████████| 357/357 [09:05<00:00,  1.53s/it]\n",
            "Epoch 16/30, Loss: 2.4531: 100%|██████████| 357/357 [09:06<00:00,  1.53s/it]\n",
            "Epoch 17/30, Loss: 2.2363: 100%|██████████| 357/357 [09:06<00:00,  1.53s/it]\n",
            "Epoch 18/30, Loss: 2.1543: 100%|██████████| 357/357 [09:09<00:00,  1.54s/it]\n",
            "Epoch 19/30, Loss: 1.9775: 100%|██████████| 357/357 [09:06<00:00,  1.53s/it]\n",
            "Epoch 20/30, Loss: 2.0859: 100%|██████████| 357/357 [08:56<00:00,  1.50s/it]\n",
            "Epoch 21/30, Loss: 2.0195: 100%|██████████| 357/357 [09:04<00:00,  1.52s/it]\n",
            "Epoch 22/30, Loss: 2.3789: 100%|██████████| 357/357 [08:59<00:00,  1.51s/it]\n",
            "Epoch 23/30, Loss: 2.0469: 100%|██████████| 357/357 [08:58<00:00,  1.51s/it]\n",
            "Epoch 24/30, Loss: 2.0312: 100%|██████████| 357/357 [09:09<00:00,  1.54s/it]\n",
            "Epoch 25/30, Loss: 2.1973: 100%|██████████| 357/357 [08:59<00:00,  1.51s/it]\n",
            "Epoch 26/30, Loss: 2.4199: 100%|██████████| 357/357 [09:07<00:00,  1.53s/it]\n",
            "Epoch 27/30, Loss: 2.0742: 100%|██████████| 357/357 [09:08<00:00,  1.54s/it]\n",
            "Epoch 28/30, Loss: 2.0156: 100%|██████████| 357/357 [09:09<00:00,  1.54s/it]\n",
            "Epoch 29/30, Loss: 2.1250: 100%|██████████| 357/357 [09:11<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Loss: 3.8397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test dataloader\n",
        "json_path = '/content/Indofashionclip/test_data.json'\n",
        "image_path = '/content/Indofashionclip/images/test/'\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    input_data = []\n",
        "    for line in f:\n",
        "        obj = json.loads(line)\n",
        "        input_data.append(obj)\n",
        "\n",
        "list_image_path = []\n",
        "list_txt = []\n",
        "for item in input_data:\n",
        "  img_path = image_path + item['image_path'].split('/')[-1]\n",
        "  caption = item['class_label'][:40]\n",
        "  list_image_path.append(img_path)\n",
        "  list_txt.append(caption)\n",
        "\n",
        "dataset = image_title_dataset(list_image_path, list_txt)\n",
        "test_dataloader = DataLoader(dataset, batch_size=256, shuffle=True) # Define test dataloader"
      ],
      "metadata": {
        "id": "p8BP0nsb1smm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "test_loss, test_acc_img, test_acc_txt = validate(model, test_dataloader, loss_img, loss_txt, device) # test 함수랑 validate 함수랑 같음\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy - Image: {test_acc_img:.4f}, Text: {test_acc_txt:.4f}\")"
      ],
      "metadata": {
        "id": "V9haIOyX9Gua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Section\n",
        "path = \"/content/Indofashionclip/images/test/7500.jpeg\" # Set your path\n",
        "image = Image.open(path)\n",
        "\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "indo_classes = ['saree', 'blouse', 'dhoti_pants', 'dupattas', 'gowns', 'kurta_men', 'leggings_and_salwars', 'lehenga', 'mojaris_men', 'mojaris_women', 'nehru_jackets', 'palazzos', 'petticoats', 'sherwanis', 'women_kurta']\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in indo_classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{indo_classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ],
      "metadata": {
        "id": "TIIFez0SSxnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6yDNspJmIuE"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # This code is to empty cuda memory if it occurs out of memory issue when fine-tuning"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f38def8139846c3809fb86c0eda9aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02c906e1302e44e88e2501e9968ffb5d",
              "IPY_MODEL_9e5faefc33544acb931b773784d70804"
            ],
            "layout": "IPY_MODEL_d68b01bfcfef4499bdb90b6e367d0b00"
          }
        },
        "02c906e1302e44e88e2501e9968ffb5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1cc08c1bc514f9085bb6a9eda3fcaac",
            "placeholder": "​",
            "style": "IPY_MODEL_cbc3b0a01d494442bab92258091021ed",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "9e5faefc33544acb931b773784d70804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16d3fbc3f73f494da9bedb184700387d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b29859dd9e3f4ae4ad4b5b9bafe39aff",
            "value": 1
          }
        },
        "d68b01bfcfef4499bdb90b6e367d0b00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1cc08c1bc514f9085bb6a9eda3fcaac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbc3b0a01d494442bab92258091021ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16d3fbc3f73f494da9bedb184700387d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29859dd9e3f4ae4ad4b5b9bafe39aff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
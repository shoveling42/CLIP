{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfdp3Ir+L/5yNcg7vHsI+q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers torch tqdm\n","!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"acNy9hh4QnqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/openai/CLIP.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQ0JfxGeQ_29","executionInfo":{"status":"ok","timestamp":1699620331101,"user_tz":-540,"elapsed":1225,"user":{"displayName":"이준석","userId":"16355322634069492720"}},"outputId":"089550cc-e44d-48de-d682-f6a745fc2396"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CLIP'...\n","remote: Enumerating objects: 251, done.\u001b[K\n","remote: Counting objects: 100% (8/8), done.\u001b[K\n","remote: Compressing objects: 100% (8/8), done.\u001b[K\n","remote: Total 251 (delta 3), reused 3 (delta 0), pack-reused 243\u001b[K\n","Receiving objects: 100% (251/251), 8.93 MiB | 26.19 MiB/s, done.\n","Resolving deltas: 100% (127/127), done.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"96wTB56rOEh3","executionInfo":{"status":"ok","timestamp":1699620360388,"user_tz":-540,"elapsed":9494,"user":{"displayName":"이준석","userId":"16355322634069492720"}},"outputId":"9b3290b7-843b-4964-fdbf-38aae8f14377"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label probs: [[0.99279356 0.00421069 0.00299575]]\n"]}],"source":["import torch\n","import clip\n","from PIL import Image\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","image = preprocess(Image.open(\"/content/CLIP/CLIP.png\")).unsqueeze(0).to(device) # set your CLIP.png path\n","text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image)\n","    text_features = model.encode_text(text)\n","\n","    logits_per_image, logits_per_text = model(image, text)\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"]}]}